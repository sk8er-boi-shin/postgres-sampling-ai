# 目次
 
1. [機能要件](#1機能要件)  
2. [非機能要件](#2非機能要件)  
3. [技術選定（使用技術と実行環境）](#3技術選定使用技術と実行環境) 

📘 **本ドキュメントで使用する用語や技術的な前提知識については、[参考資料・用語集](./03_reference.md#前提知識) を参照してください。**

---
# 1.機能要件

本ツールが満たすべき機能要件は以下の通りである。

### 1. サンプリング行数の推定
- 各テーブルに対し、過去の実行ログや統計情報をもとに最適なサンプリング行数を推定する。
- 初期フェーズではルールベースまたは回帰モデルを用い、将来的には強化学習への対応も視野に入れる。

### 2. メトリクスの収集

本ツールでは、初期段階として以下の統計情報およびメタ情報を PostgreSQL から収集対象とする：

- 使用するシステムカタログ：
  - `pg_stat_all_tables`
  - `pg_stat_user_indexes`
  - `pg_class`
  - `pg_attribute`
  - `pg_stats`

- 収集する主な項目：
  - テーブル行数（`reltuples`）
  - 空き領域や断片化の程度（`n_dead_tup`）
  - カーディナリティ、相関係数、`n_distinct` などの列単位の情報

なお、今後の実装過程や性能検証を通じて、必要に応じて他のカタログビューや内部統計情報の収集も視野に入れている。たとえば `pg_stat_progress_analyze` や `pg_stat_io`、拡張統計情報（extended statistics）などが対象候補として挙げられる。


### 3. ANALYZEの実行制御
- 推定された行数に応じて `ANALYZE` を動的に実行する。
- パーティションテーブルや部分インデックスを考慮し、適切なスコープで `ANALYZE` を発行する。
- 対象外テーブル（foreign tablesなど）はスキップする。

### 4. ログ記録および異常検知
- 実行結果（推定値、実行時間、更新結果など）をログとして記録する。
- エラー発生時には適切にログ出力し、再実行やスキップ処理を実装する。

### 5. 設定ファイルおよびパラメータ制御
- ユーザーが対象スキーマや対象外テーブル、閾値などを設定ファイルで指定できるようにする。
- 実行モード（dry-run、実行モードなど）を切り替え可能にする。

### 6. スケジューラやCI/CDとの連携（将来的機能）
- cronやジョブスケジューラとの連携を想定したバッチ実行機能。
- CI/CDツールと連携した自動化の仕組みの構築。



---


# 2.非機能要件

本ツールの非機能要件は、主に実行効率・保守性・運用性・拡張性を考慮して定義される。  
また、基本設計段階で必要となる各種しきい値や制約条件についても初期値を設け、将来的な自動調整や運用最適化の基盤とする。

### 1. パフォーマンス

- 大規模なテーブルに対しても、数分以内で統計情報の収集と更新が完了することを目指す。
- サンプリングによる高速な統計収集を前提とし、必要に応じてフルスキャンとの比較も行う。
- 実行時のメモリ消費量やCPU使用率を監視し、過剰なリソース消費を回避する。

#### 初期しきい値（パフォーマンス関連）

| 項目 | 説明 | 初期値（例） |
|------|------|-------------|
| 最大許容実行時間(ANALYZE) | 1テーブルの処理時間上限。これを超えた場合は警告ログを出力し、処理自体は継続する。※1 | 60秒 |
| メモリ使用上限 | プロセス単位のメモリ制限 | 1GB |
| 並列実行数 | 同時ANALYZE数 | 1～4スレッド |

※1：今後の検討により、処理中断（キャンセル）や通知レベルの変更（エラー扱い）を選択可能にするオプションの追加も視野に入れる。



### 2. 可用性・安定性

- エラー発生時にはログを記録しつつ、安全に処理を終了・スキップする機構を持つ。
- 予期しない異常終了を防ぐフェールセーフ機構を実装する。

#### 初期しきい値（異常検知・リトライ）

| 項目 | 説明 | 初期値（例） |
|------|------|-------------|
| リトライ回数 | 一時的な失敗の再試行 | 最大3回 |
| 統計収集失敗時の対応 | 無効な統計が返った場合 | スキップ＋ログ出力 |



### 3. 保守性

- モジュール単位での変更が容易な構成とする。
- ログ出力やエラー内容を確認しやすく、トラブルシューティングが容易であること。
- ※ モジュール構成やinterface分離方針、DIの適用範囲などは、詳細設計にて補足予定。


### 4. 拡張性

- PostgreSQL以外のRDBMSへの対応や、新たな統計項目の追加が可能な構成とする。
- 将来的なWeb UI連携や外部API連携に備えたモジュール分離を意識した設計とする。
- ※ 拡張を見越した抽象化構成やインタフェース定義方針等は、詳細設計にて補足予定。


### 5. 実行環境と互換性

- 初期リリースは PostgreSQL 15系 + Linux環境 でのローカル実行を前提とする。
- 将来的にクラウド環境（例：AWS RDS、GCP Cloud SQL）への対応も視野に入れる。



### 6. セキュリティ

| 項目 | 内容 | 備考 |
|------|------|------|
| SQLインジェクション対策 | 全てのクエリでバインド変数を使用。文字列連結によるSQL構築を禁止。 | GUI展開時は特に注意が必要なため、共通API層で制御を徹底。 |
| サーバパラメータ制限 | `SET`文などでセッション単位のパラメータ変更が行われないよう制限。 | 接続ユーザーの権限で制御。GUI経由でも無効化する設計とする。 |
| 危険コマンド制御 | `COPY`, `DO`, `EXECUTE`などの実行を制限する。 | フロントエンドから渡されるコマンドもバックエンドで検証。 |
| 長時間クエリ対策 | `statement_timeout`の設定により、暴走クエリを防止。 | GUI経由での実行時もタイムアウトを明示的に設ける予定。 |

※ 将来的にGUIによる操作機能を実装予定であり、その際は不正な入力や想定外の操作によるリスクが増加する可能性がある。よって、初期段階からセキュリティ方針を明文化し、バックエンドでの制限を前提とした設計を行う。GUI実装時にはこれらのポリシーに基づき、入力チェック、API認可、ロール制御などを強化予定。



### 7. ロギング・監査

- 実行結果、処理対象、処理時間、推定サンプリング値などをログとして記録。
- 後続分析やパフォーマンス評価のためのCSV出力なども検討する。

#### 初期しきい値（ログ関連）

| 項目 | 説明 | 初期値（例） |
|------|------|--------------|
| 実行ログ出力条件 | クエリ実行時間に応じてログ出力 | 5秒超過時のみ出力（※1） |

※1：初期状態では5秒をスロークエリのしきい値とし、それを超えるクエリのみログ出力。将来的には動的調整や平均応答時間に応じたしきい値制御の拡張も想定。また、以下の条件では実行時間に関係なくログ出力を行う：  
・クエリ実行時にエラーが発生した場合  
・統計精度劣化が検出された場合（例：想定されていた `Index Scan` が `Seq Scan` や `Bitmap Heap Scan` に変化した場合、推定コストが過去実績に比べて大幅に上昇した場合など）

#### ▼ 統計精度劣化の検出例（参考）：
- **スキャン方式の変化**  
  例：`Index Scan` → `Seq Scan` / `Bitmap Heap Scan` への変更
- **推定コストの急増**  
  過去の `total cost` や実行時間と比較して倍以上などの増加
- **実行計画上の行数推定誤差**  
  `rows=10` の想定に対して実際は `10000 rows` 返るケースなど（選択性の誤推定）
- **急激な実行時間の増加**  
  クエリ自体が変わっていないにも関わらず、実行時間が数倍に増加


### 8. テーブル選定条件・サンプリング設定

- テーブルの行数やスキーマ、種別によって処理対象を選定する。
- サンプリング率や対象除外条件は、初期値を設けるが将来的にAIで動的に調整予定。

#### 初期しきい値（選定・サンプリング関連）

| 項目 | 説明 | 初期値（例） |
|------|------|-------------|
| 最小行数閾値 | 対象とする最低行数 | 設定せず（※1） |
| 除外スキーマ名 | 処理対象から除外 | pg_catalog, information_schema |
| 外部テーブル除外 | 外部テーブルと判定される属性（※2） | 除外対象 |
| サンプリング行数 | ANALYZEで使用されるサンプル数 | PostgreSQLの統計設定（※3）に基づき自動調整 |
| default_statistics_target | 列単位統計の粒度 | 100（PostgreSQL初期値） |
| 統計更新頻度制限 | 再実行を避ける調整 | 1時間 or 1日単位で間引き可（手動設定可）※4 |

※1：データ量が大幅に減少（例：1万件→5千件など）した場合にも統計情報の更新対象としたいため、初期段階ではあえて固定の最小行数閾値は設けない。将来的にはデータ変動量やAIによる動的判定ロジックの導入を想定しており、そのため柔軟性を重視した設計としている。

※2：PostgreSQLでは、`pg_class.relkind` が `'f'` の場合、そのテーブルは「外部テーブル（foreign table）」として定義されている。fdw（foreign data wrapper）経由で外部ソースに接続しているテーブルのため、統計情報の更新対象から除外する。

※3：PostgreSQLでは、`default_statistics_target` の設定値や列数、統計の種類などをもとに、ANALYZE時に必要なサンプリング行数が内部的に決定されます。本ツールではこの仕組みを前提としつつ、必要に応じて調整を検討します。また、将来的にAIによる調整も可能とする設計とします。

※4：将来的には、pg_stat_xxx やクエリ実行時間などを常時監視し、パフォーマンス劣化の兆候をもとに、統計情報の更新タイミングをAI的に自動制御する仕組みへの拡張を想定。


### 備考と補足

- 各しきい値は「初期値」であり、**運用やAIによる動的最適化**を前提とする。
- バージョン・実行環境（オンプレ／クラウド）により変動しやすいため、**環境変数や設定ファイルで上書き可能**にしておくことが望ましい。


---


# 3.技術選定（使用技術と実行環境）

本ツールの実装にあたっては、以下の観点を踏まえて技術選定を行っている：

- 実行効率と学習コストのバランス
- データ分析・モデル構築に適したライブラリの充実性
- PostgreSQLとの親和性、およびSQLとの連携のしやすさ
- 将来的なWeb UIやクラウド展開の可能性


### 使用技術

| 分類 | 技術 | 用途・理由 |
|------|------|-------------|
| 言語 | Python 3.9+ | データ処理・AIモデル構築・DBアクセスの全てを一貫して実装可能。学習コストとライブラリの豊富さを両立。※1 |
| データ分析 | pandas / numpy | 統計情報や実行ログの加工・分析処理に使用。処理対象が軽量なため、pandas / numpy を選定。※2 |
| 機械学習   | scikit-learn   | 統計情報からサンプリング行数を予測する軽量な回帰モデルの構築に使用。構造がシンプルで小〜中規模データに強く、PoCとしての実装・保守性のバランスが良好なため選定。※3 |
| データ可視化（任意） | matplotlib / seaborn | モデル精度や特徴量分布の確認用途（PoC段階）※4 |
| データベース接続 | psycopg2 | PostgreSQLと安全に連携可能なDBドライバとして採用。 |
| ログ管理 | logging | 実行結果やエラー情報をログ出力。学習用データの蓄積にも利用。 |

※1：RやJuliaはデータ処理や統計解析に優れるが、DBアクセスやバッチ処理との統合性に課題あり。JavaやC#は業務システム向きだが、AI/MLライブラリの使いやすさや学習コストの面で不利。Pythonは機械学習・データ分析・データベース処理の全てにバランス良く対応可能。

※2：クエリ1件あたりの処理対象が小規模（最大100件程度）のため、起動の軽さと成熟したエコシステムを重視して pandas / numpy を選定した。  
処理回数は多くても1回あたりの処理が軽量なため、性能と保守性のバランスが良い。  
Dask や Polars などの高速処理系ライブラリは、大規模データや並列処理向けに設計されており、  
本用途では初期化やスケジューラ起動によるオーバーヘッドが逆に負担となる。文法や保守の観点からも、今回のPoCには適さないと判断した。

※3：本ツールでは、**1回の実行計画取得で扱う特徴量が数十列以下、データ量が数千件程度と小〜中規模**であるため、**軽量で高速な scikit-learn を選定**した。pandas / numpy との親和性や、`.fit()` / `.predict()` の手軽さ、モデルの保存・再利用が容易な点も利点。
XGBoost や LightGBM は高精度だが、本用途では**初期化やチューニングの負荷が高くオーバースペック**。TensorFlow・PyTorch などの深層学習系も、**構築や保守の複雑さ、解釈性の低さから不向き**と判断した。

※4：本ツールでは、モデル精度や特徴量分布の可視化目的で、軽量かつ成熟した matplotlib / seaborn を選定。  
pandas や scikit-learn との親和性が高く、PoC段階で必要な可視化要件を十分に満たせる。  
Plotly や Altair、Dash などの高度な可視化ツールはオーバースペックとなるため、採用しなかった。


### 実行環境

| 区分 | 内容 |
|------|------|
| OS | Linux（Ubuntu 22.04）想定。バッチ処理・データ分析に適した軽量かつ安定した実行環境を提供し、Python / PostgreSQL との親和性・クラウド移行性にも優れる。 macOSも動作対象。 |
| Pythonバージョン | Python 3.9 以上 （主要ライブラリの対応・保守性を考慮）※1|
| PostgreSQLバージョン | PostgreSQL 13 以上（ANALYZE 処理や統計情報管理の機能改善を含む） |
| 実行形式 | ローカルスクリプト（CLI）ベース。cron等による定期実行も可能。 |
| 実行条件 | データベースに接続可能な権限（ANALYZE権限を含む）を持つこと。 |
| 拡張前提 | 将来的に Docker 化、クラウド実行（例：AWS Batch / Lambda）も視野に入れる。 |

※1：Python 3.8以前はすでにセキュリティサポートが終了しており、長期運用の観点からは適さない。また、3.9以降では辞書のマージ演算子 `|` など可読性・保守性を高める新構文が導入されており、学習効率や開発体験の面でも優位性があるため選定。

### 今後の拡張候補

- モデル管理基盤として **MLflow** や **DVC** の導入検討
- 将来的なWeb UI との連携については、REST API を通じて任意のフロントエンド（例：React, Next.js など）と統合できる構成を前提としている。
- モデル自動更新や通知を含む **CI/CD パイプライン連携**


