📌 **This document is planned to be split into smaller sections upon finalization for readability.**


# 目次

1. [はじめに](#はじめに)  
2. [背景と目的](#背景と目的)  
3. [対象範囲](#対象範囲)  
4. [全体構成](#全体構成)  
5. [機能要件](#機能要件)  
6. [非機能要件](#非機能要件)  
7. [技術選定（使用技術と実行環境）](#技術選定使用技術と実行環境)  
8. [処理概要](#処理概要)  
9. [入出力仕様](#入出力仕様)  
10. [モジュール構成](#モジュール構成)  
11. [考慮すべきデータ構造と仕様](#考慮すべきデータ構造と仕様)  
12. [処理フローとロジック概要](#処理フローとロジック概要)  
13. [外部インターフェース（DB/API等）](#外部インターフェースdbapi等)  
14. [AIによるサンプリング行数の推定ロジック](#aiによるサンプリング行数の推定ロジック)  
15. [エラーハンドリング・ロギング](#エラーハンドリングロギング)  
16. [今後の拡張案](#今後の拡張案)  
17. [参考資料・用語集](#参考資料用語集)




---
# はじめに

本ドキュメントは、PostgreSQLにおける統計情報の更新処理に対して、AIを用いて最適なサンプリング行数を自動算出し、処理効率を向上させるツールの基本設計をまとめたものである。

PostgreSQLではクエリ最適化のために統計情報が使用されるが、その更新には `ANALYZE` の実行が必要であり、対象テーブルの規模や特性によって処理時間や精度に大きく差が出る。本プロジェクトでは、以下のような課題に対処するための仕組みを設計する。

- 大規模テーブルに対する ANALYZE の長時間実行
- 統計情報の不整合による実行計画の非最適化
- 更新頻度やデータ特性に応じた最適なサンプリング戦略の不足
- パーティションテーブルや断片化されたテーブルへの対応
- `pg_stat` 系情報の遅延反映や推定値によるブレの吸収


本設計書では、これらの問題を解決するためのシステム構成、処理フロー、入出力仕様、そしてサンプリング行数をAIにより動的に算出するロジックの設計について詳細に述べる。

また、本ツールはPoC（Proof of Concept）としてローカル実行可能なスクリプトから開始し、将来的にはクラウド環境でのスケーラブルな実行や、Web UIとの連携を視野に入れている。


---

# 背景と目的

PostgreSQLでは、クエリの最適な実行計画を生成するために、テーブルごとの統計情報が利用される。  

しかし、これらの統計情報は `ANALYZE` により手動または自動で収集される必要があり、
その際にはテーブル全体からランダムにサンプリングされた行に対して統計を収集する。

サンプリングされる行数は PostgreSQL が内部的に計算しており、
通常は各列に対して `default_statistics_target` の値（デフォルトでは100）を基準に必要な数だけ選ばれる。

この値は列ごとに変更可能であり、精度と処理負荷のバランスを調整できるが、
テーブルの規模や構造によっては、サンプリングであっても一定の処理負荷や実行時間がかかるという課題がある。


また、PostgreSQLの自動ANALYZEは、トリガー条件（更新率やタイミング）に依存しており、実際の運用環境では以下のような問題が発生しやすい：

- 更新頻度やデータ特性に応じた最適なサンプリング戦略が存在しない
- フルスキャンとサンプリングの使い分けに明確な指針がない
- ユーザーや運用担当者による属人的なANALYZE実行の判断

さらに、パーティションテーブルや断片化が進行したテーブルにおいては、統計情報の不整合が発生しやすく、実行計画の選定ミスを招く要因となる。

そこで本プロジェクトでは、AIを用いてテーブルごとの特性（件数、更新率、カーディナリティ、ユニーク度など）を動的に分析し、最適なサンプリング行数を自動で算出する仕組みを開発する。

本ツールにより、以下のような価値を提供することを目的とする：

- 統計情報の収集時間を短縮し、ANALYZEの運用効率を向上
- 実データに即したサンプリング行数を推定し、実行計画の精度向上を図る
- 手動調整や試行錯誤を減らし、統計情報の運用自動化を実現

この設計書では、上記の課題を解決するための構成方針、実行ロジック、AIによる推定アプローチ、考慮すべき仕様について述べる。


---

# 対象範囲

本設計における対象範囲は以下のとおりである。

### 対象とする処理範囲

- PostgreSQL データベースに対する統計情報の更新 (`ANALYZE`) 処理
- `default_statistics_target` を基準としたサンプリング行数の最適化
- 対象テーブルごとに動的なサンプリング行数を算出し、`ANALYZE` を実行するスクリプトの設計
- 通常のローカルテーブルおよびパーティションテーブルの統計情報更新

### 対象外とする項目

- 外部テーブル（Foreign Tables）やFDWを介したテーブルへの対応
- PostgreSQL 以外の RDBMS（MySQL, Oracle, SQL Server など）への対応  
  ※現時点では PostgreSQL のみを対象とするが、本プロジェクトの成果が安定・有効と判断されれば、他の主要RDBMSへの拡張も将来的な検討対象とする。
- 自動統計情報更新の PostgreSQL 本体の挙動変更や、サーバパラメータの制御
- Web UI やダッシュボードによる可視化機能（将来的な拡張対象）

### 想定環境

- PostgreSQL 13 以上を対象とし、特に `parallel analyze` の恩恵を受けられるバージョンを想定
- 開発・実行環境はローカルPCまたは軽量VPS
- 本番環境への導入は将来的なPoC結果に基づき判断


---
# 全体構成

本ツールは、PostgreSQLの統計情報更新に関わるパフォーマンスを改善することを目的とし、  
AIによる最適なサンプリング行数の推定と、それに基づく `ANALYZE` 処理を実行するアプリケーションである。

初期フェーズではローカル実行環境を想定し、将来的にはクラウド環境やWeb UIとの統合も視野に入れている。

本ツールは、以下の2つのフェーズで構成される：

- **学習フェーズ**：クエリ実行前後の統計情報・実行計画を収集し、最適なサンプリング行数をAIで学習
- **適用フェーズ**：学習済みモデルを使用して、推定されたサンプリング行数で本番環境にANALYZEを適用


## 学習フェーズと適用フェーズの処理概要

```
[学習フェーズ：開発環境やPoCで実行]
↓
1. クエリを指定
↓
2. 含まれるテーブルごとに以下を繰り返し実行：
   - 統計情報メトリクスの取得（pg_stat等）
   - AIによるサンプリング行数の推定
   - 推定結果に基づく ANALYZE の実行
   - 実行計画の取得と処理時間の記録
↓
3. 実行計画や統計情報をもとに AI を再学習
↓
4. 学習済みモデルをファイルに保存（例：best_model.pkl）

↓

[適用フェーズ：本番環境などで実行]
↓
1. クエリを指定
↓
2. 含まれるテーブルのメトリクスのみを取得
↓
3. 学習済みモデルでサンプリング行数を推定
↓
4. 推定値に基づいて ANALYZE を実行
   （または dry-run モードでログ出力）
```




## コンポーネント構成

以下に、学習フェーズおよび適用フェーズを支えるモジュール構成を示す：

### 学習フェーズにおける構成

#### 1. サンプリング最適化エンジン（AIロジック部）
- 過去の実行ログや統計情報をもとに、各テーブルに対する適切なサンプリング行数を推定
- 現在は単純なルールベースまたは回帰モデルを採用し、将来的に強化学習やクラスタリングへの展開も視野に入れる
- 学習済みモデルはファイル（例：Pickle形式）として保存され、適用フェーズで再利用される

#### 2. メトリクス取得モジュール
- `pg_stat_all_tables`、`pg_stat_user_indexes`、`pg_class` などのシステムカタログから統計・構造情報を収集
- `reltuples`、`n_dead_tup`、カーディナリティ、相関係数などを収集対象とする
- クエリに含まれるすべてのテーブルが対象となる

#### 3. 統計更新・実行計画収集モジュール
- 推定されたサンプリング行数をもとに `ANALYZE` を実行し、統計情報を更新
- 続いて対象クエリを実行し、得られた実行計画と実行時間を記録
- 結果は履歴管理テーブルまたはログファイルに保存され、学習に再利用される


### 適用フェーズにおける構成

#### 4. 学習済みモデル適用モジュール
- 学習済みモデルファイルを読み込み、対象テーブルに対してサンプリング行数を推定
- モデルと特徴量に不整合がある場合は既定値にフォールバックする設計とする（フェールセーフ）

#### 5. サンプリング実行・統計更新モジュール（本番適用）
- 推定されたサンプリング行数に従って、`ANALYZE` を実行
- 対象はクエリに含まれるテーブルに限定される
- 統計情報の更新後、実行計画を取得することなく、本番処理を継続可能

### 共通機能

#### 6. ログ・エラーハンドリング機構
- 実行結果や異常検知のログ出力
- エラー発生時のリトライ処理、対象除外、学習フェーズへのフィードバック処理を含む

#### 7. （将来的な構想）Web UI / API連携モジュール
- クエリパターンや予測モデルの可視化
- サンプリング精度や効果の分析画面
- バックエンドAPI連携によるCI/CD統合の検討



---


# 機能要件

本ツールが満たすべき機能要件は以下の通りである。

### 1. サンプリング行数の推定
- 各テーブルに対し、過去の実行ログや統計情報をもとに最適なサンプリング行数を推定する。
- 初期フェーズではルールベースまたは回帰モデルを用い、将来的には強化学習への対応も視野に入れる。

### 2. メトリクスの収集

本ツールでは、初期段階として以下の統計情報およびメタ情報を PostgreSQL から収集対象とする：

- 使用するシステムカタログ：
  - `pg_stat_all_tables`
  - `pg_stat_user_indexes`
  - `pg_class`
  - `pg_attribute`
  - `pg_stats`

- 収集する主な項目：
  - テーブル行数（`reltuples`）
  - 空き領域や断片化の程度（`n_dead_tup`）
  - カーディナリティ、相関係数、`n_distinct` などの列単位の情報

なお、今後の実装過程や性能検証を通じて、必要に応じて他のカタログビューや内部統計情報の収集も視野に入れている。たとえば `pg_stat_progress_analyze` や `pg_stat_io`、拡張統計情報（extended statistics）などが対象候補として挙げられる。


### 3. ANALYZEの実行制御
- 推定された行数に応じて `ANALYZE` を動的に実行する。
- パーティションテーブルや部分インデックスを考慮し、適切なスコープで `ANALYZE` を発行する。
- 対象外テーブル（foreign tablesなど）はスキップする。

### 4. ログ記録および異常検知
- 実行結果（推定値、実行時間、更新結果など）をログとして記録する。
- エラー発生時には適切にログ出力し、再実行やスキップ処理を実装する。

### 5. 設定ファイルおよびパラメータ制御
- ユーザーが対象スキーマや対象外テーブル、閾値などを設定ファイルで指定できるようにする。
- 実行モード（dry-run、実行モードなど）を切り替え可能にする。

### 6. スケジューラやCI/CDとの連携（将来的機能）
- cronやジョブスケジューラとの連携を想定したバッチ実行機能。
- CI/CDツールと連携した自動化の仕組みの構築。



---


# 非機能要件

本ツールの非機能要件は、主に実行効率・保守性・運用性・拡張性を考慮して定義される。  
また、基本設計段階で必要となる各種しきい値や制約条件についても初期値を設け、将来的な自動調整や運用最適化の基盤とする。

## 1. パフォーマンス

- 大規模なテーブルに対しても、数分以内で統計情報の収集と更新が完了することを目指す。
- サンプリングによる高速な統計収集を前提とし、必要に応じてフルスキャンとの比較も行う。
- 実行時のメモリ消費量やCPU使用率を監視し、過剰なリソース消費を回避する。

### 初期しきい値（パフォーマンス関連）

| 項目 | 説明 | 初期値（例） |
|------|------|-------------|
| 最大許容実行時間(ANALYZE) | 1テーブルの処理時間上限。これを超えた場合は警告ログを出力し、処理自体は継続する。※1 | 60秒 |
| メモリ使用上限 | プロセス単位のメモリ制限 | 1GB |
| 並列実行数 | 同時ANALYZE数 | 1～4スレッド |

※1：今後の検討により、処理中断（キャンセル）や通知レベルの変更（エラー扱い）を選択可能にするオプションの追加も視野に入れる。



## 2. 可用性・安定性

- エラー発生時にはログを記録しつつ、安全に処理を終了・スキップする機構を持つ。
- 予期しない異常終了を防ぐフェールセーフ機構を実装する。

### 初期しきい値（異常検知・リトライ）

| 項目 | 説明 | 初期値（例） |
|------|------|-------------|
| リトライ回数 | 一時的な失敗の再試行 | 最大3回 |
| 統計収集失敗時の対応 | 無効な統計が返った場合 | スキップ＋ログ出力 |



## 3. 保守性

- モジュール単位での変更が容易な構成とする。
- ログ出力やエラー内容を確認しやすく、トラブルシューティングが容易であること。
- ※ モジュール構成やinterface分離方針、DIの適用範囲などは、詳細設計にて補足予定。


## 4. 拡張性

- PostgreSQL以外のRDBMSへの対応や、新たな統計項目の追加が可能な構成とする。
- 将来的なWeb UI連携や外部API連携に備えたモジュール分離を意識した設計とする。
- ※ 拡張を見越した抽象化構成やインタフェース定義方針等は、詳細設計にて補足予定。


## 5. 実行環境と互換性

- 初期リリースは PostgreSQL 15系 + Linux環境 でのローカル実行を前提とする。
- 将来的にクラウド環境（例：AWS RDS、GCP Cloud SQL）への対応も視野に入れる。



## 6. セキュリティ

| 項目 | 内容 | 備考 |
|------|------|------|
| SQLインジェクション対策 | 全てのクエリでバインド変数を使用。文字列連結によるSQL構築を禁止。 | GUI展開時は特に注意が必要なため、共通API層で制御を徹底。 |
| サーバパラメータ制限 | `SET`文などでセッション単位のパラメータ変更が行われないよう制限。 | 接続ユーザーの権限で制御。GUI経由でも無効化する設計とする。 |
| 危険コマンド制御 | `COPY`, `DO`, `EXECUTE`などの実行を制限する。 | フロントエンドから渡されるコマンドもバックエンドで検証。 |
| 長時間クエリ対策 | `statement_timeout`の設定により、暴走クエリを防止。 | GUI経由での実行時もタイムアウトを明示的に設ける予定。 |

※ 将来的にGUIによる操作機能を実装予定であり、その際は不正な入力や想定外の操作によるリスクが増加する可能性がある。よって、初期段階からセキュリティ方針を明文化し、バックエンドでの制限を前提とした設計を行う。GUI実装時にはこれらのポリシーに基づき、入力チェック、API認可、ロール制御などを強化予定。



## 7. ロギング・監査

- 実行結果、処理対象、処理時間、推定サンプリング値などをログとして記録。
- 後続分析やパフォーマンス評価のためのCSV出力なども検討する。

### 初期しきい値（ログ関連）

| 項目 | 説明 | 初期値（例） |
|------|------|--------------|
| 実行ログ出力条件 | クエリ実行時間に応じてログ出力 | 5秒超過時のみ出力（※1） |

※1：初期状態では5秒をスロークエリのしきい値とし、それを超えるクエリのみログ出力。将来的には動的調整や平均応答時間に応じたしきい値制御の拡張も想定。また、以下の条件では実行時間に関係なくログ出力を行う：  
・クエリ実行時にエラーが発生した場合  
・統計精度劣化が検出された場合（例：想定されていた `Index Scan` が `Seq Scan` や `Bitmap Heap Scan` に変化した場合、推定コストが過去実績に比べて大幅に上昇した場合など）

#### ▼ 統計精度劣化の検出例（参考）：
- **スキャン方式の変化**  
  例：`Index Scan` → `Seq Scan` / `Bitmap Heap Scan` への変更
- **推定コストの急増**  
  過去の `total cost` や実行時間と比較して倍以上などの増加
- **実行計画上の行数推定誤差**  
  `rows=10` の想定に対して実際は `10000 rows` 返るケースなど（選択性の誤推定）
- **急激な実行時間の増加**  
  クエリ自体が変わっていないにも関わらず、実行時間が数倍に増加


## 8. テーブル選定条件・サンプリング設定

- テーブルの行数やスキーマ、種別によって処理対象を選定する。
- サンプリング率や対象除外条件は、初期値を設けるが将来的にAIで動的に調整予定。

### 初期しきい値（選定・サンプリング関連）

| 項目 | 説明 | 初期値（例） |
|------|------|-------------|
| 最小行数閾値 | 対象とする最低行数 | 設定せず（※1） |
| 除外スキーマ名 | 処理対象から除外 | pg_catalog, information_schema |
| 外部テーブル除外 | 外部テーブルと判定される属性（※2） | 除外対象 |
| サンプリング行数 | ANALYZEで使用されるサンプル数 | PostgreSQLの統計設定（※3）に基づき自動調整 |
| default_statistics_target | 列単位統計の粒度 | 100（PostgreSQL初期値） |
| 統計更新頻度制限 | 再実行を避ける調整 | 1時間 or 1日単位で間引き可（手動設定可）※4 |

※1：データ量が大幅に減少（例：1万件→5千件など）した場合にも統計情報の更新対象としたいため、初期段階ではあえて固定の最小行数閾値は設けない。将来的にはデータ変動量やAIによる動的判定ロジックの導入を想定しており、そのため柔軟性を重視した設計としている。

※2：PostgreSQLでは、`pg_class.relkind` が `'f'` の場合、そのテーブルは「外部テーブル（foreign table）」として定義されている。fdw（foreign data wrapper）経由で外部ソースに接続しているテーブルのため、統計情報の更新対象から除外する。

※3：PostgreSQLでは、`default_statistics_target` の設定値や列数、統計の種類などをもとに、ANALYZE時に必要なサンプリング行数が内部的に決定されます。本ツールではこの仕組みを前提としつつ、必要に応じて調整を検討します。また、将来的にAIによる調整も可能とする設計とします。

※4：将来的には、pg_stat_xxx やクエリ実行時間などを常時監視し、パフォーマンス劣化の兆候をもとに、統計情報の更新タイミングをAI的に自動制御する仕組みへの拡張を想定。


## 備考と補足

- 各しきい値は「初期値」であり、**運用やAIによる動的最適化**を前提とする。
- バージョン・実行環境（オンプレ／クラウド）により変動しやすいため、**環境変数や設定ファイルで上書き可能**にしておくことが望ましい。


---


# 技術選定（使用技術と実行環境）

本ツールの実装にあたっては、以下の観点を踏まえて技術選定を行っている：

- 実行効率と学習コストのバランス
- データ分析・モデル構築に適したライブラリの充実性
- PostgreSQLとの親和性、およびSQLとの連携のしやすさ
- 将来的なWeb UIやクラウド展開の可能性


## 使用技術

| 分類 | 技術 | 用途・理由 |
|------|------|-------------|
| 言語 | Python 3.9+ | データ処理・AIモデル構築・DBアクセスの全てを一貫して実装可能。学習コストとライブラリの豊富さを両立。※1 |
| データ分析 | pandas / numpy | 統計情報や実行ログの加工・分析処理に使用。処理対象が軽量なため、pandas / numpy を選定。※2 |
| 機械学習   | scikit-learn   | 統計情報からサンプリング行数を予測する軽量な回帰モデルの構築に使用。構造がシンプルで小〜中規模データに強く、PoCとしての実装・保守性のバランスが良好なため選定。※3 |
| データ可視化（任意） | matplotlib / seaborn | モデル精度や特徴量分布の確認用途（PoC段階）※4 |
| データベース接続 | psycopg2 | PostgreSQLと安全に連携可能なDBドライバとして採用。 |
| ログ管理 | logging | 実行結果やエラー情報をログ出力。学習用データの蓄積にも利用。 |

※1：RやJuliaはデータ処理や統計解析に優れるが、DBアクセスやバッチ処理との統合性に課題あり。JavaやC#は業務システム向きだが、AI/MLライブラリの使いやすさや学習コストの面で不利。Pythonは機械学習・データ分析・データベース処理の全てにバランス良く対応可能。

※2：クエリ1件あたりの処理対象が小規模（最大100件程度）のため、起動の軽さと成熟したエコシステムを重視して pandas / numpy を選定した。  
処理回数は多くても1回あたりの処理が軽量なため、性能と保守性のバランスが良い。  
Dask や Polars などの高速処理系ライブラリは、大規模データや並列処理向けに設計されており、  
本用途では初期化やスケジューラ起動によるオーバーヘッドが逆に負担となる。文法や保守の観点からも、今回のPoCには適さないと判断した。

※3：本ツールでは、**1回の実行計画取得で扱う特徴量が数十列以下、データ量が数千件程度と小〜中規模**であるため、**軽量で高速な scikit-learn を選定**した。pandas / numpy との親和性や、`.fit()` / `.predict()` の手軽さ、モデルの保存・再利用が容易な点も利点。
XGBoost や LightGBM は高精度だが、本用途では**初期化やチューニングの負荷が高くオーバースペック**。TensorFlow・PyTorch などの深層学習系も、**構築や保守の複雑さ、解釈性の低さから不向き**と判断した。

※4：本ツールでは、モデル精度や特徴量分布の可視化目的で、軽量かつ成熟した matplotlib / seaborn を選定。  
pandas や scikit-learn との親和性が高く、PoC段階で必要な可視化要件を十分に満たせる。  
Plotly や Altair、Dash などの高度な可視化ツールはオーバースペックとなるため、採用しなかった。


## 実行環境

| 区分 | 内容 |
|------|------|
| OS | Linux（Ubuntu 22.04）想定。バッチ処理・データ分析に適した軽量かつ安定した実行環境を提供し、Python / PostgreSQL との親和性・クラウド移行性にも優れる。 macOSも動作対象。 |
| Pythonバージョン | Python 3.9 以上 （主要ライブラリの対応・保守性を考慮）※1|
| PostgreSQLバージョン | PostgreSQL 13 以上（ANALYZE 処理や統計情報管理の機能改善を含む） |
| 実行形式 | ローカルスクリプト（CLI）ベース。cron等による定期実行も可能。 |
| 実行条件 | データベースに接続可能な権限（ANALYZE権限を含む）を持つこと。 |
| 拡張前提 | 将来的に Docker 化、クラウド実行（例：AWS Batch / Lambda）も視野に入れる。 |

※1：Python 3.8以前はすでにセキュリティサポートが終了しており、長期運用の観点からは適さない。また、3.9以降では辞書のマージ演算子 `|` など可読性・保守性を高める新構文が導入されており、学習効率や開発体験の面でも優位性があるため選定。

## 今後の拡張候補

- モデル管理基盤として **MLflow** や **DVC** の導入検討
- 将来的なWeb UI との連携については、REST API を通じて任意のフロントエンド（例：React, Next.js など）と統合できる構成を前提としている。
- モデル自動更新や通知を含む **CI/CD パイプライン連携**


## 処理概要

本ツールは、PostgreSQLにおける統計情報更新処理のパフォーマンス最適化を目的とし、  
AIを用いてテーブルごとに適切なサンプリング行数を推定し、`ANALYZE` 処理に適用することで、更新時間の短縮と最適化を図る。

本ツールは、以下2つのフェーズに分かれて動作する：

- **学習フェーズ（PoC／開発環境想定）**  
　統計情報と実行計画を収集し、最適なサンプリング行数をAIで学習する

- **適用フェーズ（本番想定）**  
　学習済みモデルを用いてサンプリング行数を推定し、`ANALYZE` に適用する


```

[学習フェーズ]
↓
1. クエリを指定
↓
2. クエリに含まれる各テーブルに対して以下を実行：
   - 統計情報の取得
   - AIによるサンプリング行数の推定
   - ANALYZE の実行と統計更新
   - 実行計画と実行時間の取得・記録
↓
3. 実行結果をもとにモデルを再学習し、ファイル保存

↓

[適用フェーズ]
↓
1. クエリを指定
↓
2. テーブルごとにメトリクスを取得
↓
3. 学習済みモデルによりサンプリング行数を推定
↓
4. 推定結果に基づき ANALYZE を実行

```

※詳細なモジュール構成については、[全体構成](#全体構成)を参照。


