📌 **This document is planned to be split into smaller sections upon finalization for readability.**


# 目次

1. [はじめに](#はじめに)  
2. [背景と目的](#背景と目的)  
3. [対象範囲](#対象範囲)  
4. [全体構成](#全体構成)  
5. [機能要件](#機能要件)  
6. [非機能要件](#非機能要件)  
7. [技術選定（使用技術と実行環境）](#技術選定使用技術と実行環境)  
8. [処理概要](#処理概要)  
9. [入出力仕様](#入出力仕様)  
10. [モジュール構成](#モジュール構成)  
11. [考慮すべきデータ構造と仕様](#考慮すべきデータ構造と仕様)  
12. [処理フローとロジック概要](#処理フローとロジック概要)  
13. [外部インターフェース（DB/API等）](#外部インターフェースdbapi等)  
14. [AIによるサンプリング行数の推定ロジック](#aiによるサンプリング行数の推定ロジック)  
15. [エラーハンドリング・ロギング](#エラーハンドリングロギング)  
16. [今後の拡張案](#今後の拡張案)  
17. [参考資料・用語集](#参考資料用語集)




---
# はじめに

本ドキュメントは、PostgreSQLにおける統計情報の更新処理に対して、AIを用いて最適なサンプリング行数を自動算出し、処理効率を向上させるツールの基本設計をまとめたものである。

PostgreSQLではクエリ最適化のために統計情報が使用されるが、その更新には `ANALYZE` の実行が必要であり、対象テーブルの規模や特性によって処理時間や精度に大きく差が出る。本プロジェクトでは、以下のような課題に対処するための仕組みを設計する。

- 大規模テーブルに対する ANALYZE の長時間実行
- 統計情報の不整合による実行計画の非最適化
- 更新頻度やデータ特性に応じた最適なサンプリング戦略の不足
- パーティションテーブルや断片化されたテーブルへの対応
- `pg_stat` 系情報の遅延反映や推定値によるブレの吸収


本設計書では、これらの問題を解決するためのシステム構成、処理フロー、入出力仕様、そしてサンプリング行数をAIにより動的に算出するロジックの設計について詳細に述べる。

また、本ツールはPoC（Proof of Concept）としてローカル実行可能なスクリプトから開始し、将来的にはクラウド環境でのスケーラブルな実行や、Web UIとの連携を視野に入れている。


---

# 背景と目的

PostgreSQLでは、クエリの最適な実行計画を生成するために、テーブルごとの統計情報が利用される。  

しかし、これらの統計情報は `ANALYZE` により手動または自動で収集される必要があり、
その際にはテーブル全体からランダムにサンプリングされた行に対して統計を収集する。

サンプリングされる行数は PostgreSQL が内部的に計算しており、
通常は各列に対して `default_statistics_target` の値（デフォルトでは100）を基準に必要な数だけ選ばれる。

この値は列ごとに変更可能であり、精度と処理負荷のバランスを調整できるが、
テーブルの規模や構造によっては、サンプリングであっても一定の処理負荷や実行時間がかかるという課題がある。


また、PostgreSQLの自動ANALYZEは、トリガー条件（更新率やタイミング）に依存しており、実際の運用環境では以下のような問題が発生しやすい：

- 更新頻度やデータ特性に応じた最適なサンプリング戦略が存在しない
- フルスキャンとサンプリングの使い分けに明確な指針がない
- ユーザーや運用担当者による属人的なANALYZE実行の判断

さらに、パーティションテーブルや断片化が進行したテーブルにおいては、統計情報の不整合が発生しやすく、実行計画の選定ミスを招く要因となる。

そこで本プロジェクトでは、AIを用いてテーブルごとの特性（件数、更新率、カーディナリティ、ユニーク度など）を動的に分析し、最適なサンプリング行数を自動で算出する仕組みを開発する。

本ツールにより、以下のような価値を提供することを目的とする：

- 統計情報の収集時間を短縮し、ANALYZEの運用効率を向上
- 実データに即したサンプリング行数を推定し、実行計画の精度向上を図る
- 手動調整や試行錯誤を減らし、統計情報の運用自動化を実現

この設計書では、上記の課題を解決するための構成方針、実行ロジック、AIによる推定アプローチ、考慮すべき仕様について述べる。


---

# 対象範囲

本設計における対象範囲は以下のとおりである。

### 対象とする処理範囲

- PostgreSQL データベースに対する統計情報の更新 (`ANALYZE`) 処理
- `default_statistics_target` を基準としたサンプリング行数の最適化
- 対象テーブルごとに動的なサンプリング行数を算出し、`ANALYZE` を実行するスクリプトの設計
- 通常のローカルテーブルおよびパーティションテーブルの統計情報更新

### 対象外とする項目

- 外部テーブル（Foreign Tables）やFDWを介したテーブルへの対応
- PostgreSQL 以外の RDBMS（MySQL, Oracle, SQL Server など）への対応  
  ※現時点では PostgreSQL のみを対象とするが、本プロジェクトの成果が安定・有効と判断されれば、他の主要RDBMSへの拡張も将来的な検討対象とする。
- 自動統計情報更新の PostgreSQL 本体の挙動変更や、サーバパラメータの制御
- Web UI やダッシュボードによる可視化機能（将来的な拡張対象）

### 想定環境

- PostgreSQL 13 以上を対象とし、特に `parallel analyze` の恩恵を受けられるバージョンを想定
- 開発・実行環境はローカルPCまたは軽量VPS
- 本番環境への導入は将来的なPoC結果に基づき判断


---
# 全体構成

本ツールは、PostgreSQLの統計情報更新に関わるパフォーマンスを改善することを目的とし、  
AIによる最適なサンプリング行数の推定と、それに基づく `ANALYZE` 処理を実行するアプリケーションである。

初期フェーズではローカル実行環境を想定し、将来的にはクラウド環境やWeb UIとの統合も視野に入れている。

本ツールは、以下の2つのフェーズで構成される：

- **学習フェーズ**：クエリ実行前後の統計情報・実行計画を収集し、最適なサンプリング行数をAIで学習
- **適用フェーズ**：学習済みモデルを使用して、推定されたサンプリング行数で本番環境にANALYZEを適用


## 学習フェーズと適用フェーズの処理概要

```
[学習フェーズ：開発環境やPoCで実行]
↓
1. クエリを指定
↓
2. 含まれるテーブルごとに以下を繰り返し実行：
   - 統計情報メトリクスの取得（pg_stat等）
   - AIによるサンプリング行数の推定
   - 推定結果に基づく ANALYZE の実行
   - 実行計画の取得と処理時間の記録
↓
3. 実行計画や統計情報をもとに AI を再学習
↓
4. 学習済みモデルをファイルに保存（例：best_model.pkl）

↓

[適用フェーズ：本番環境などで実行]
↓
1. クエリを指定
↓
2. 含まれるテーブルのメトリクスのみを取得
↓
3. 学習済みモデルでサンプリング行数を推定
↓
4. 推定値に基づいて ANALYZE を実行
   （または dry-run モードでログ出力）
```




## コンポーネント構成

以下に、学習フェーズおよび適用フェーズを支えるモジュール構成を示す：

### 学習フェーズにおける構成

#### 1. サンプリング最適化エンジン（AIロジック部）
- 過去の実行ログや統計情報をもとに、各テーブルに対する適切なサンプリング行数を推定
- 現在は単純なルールベースまたは回帰モデルを採用し、将来的に強化学習やクラスタリングへの展開も視野に入れる
- 学習済みモデルはファイル（例：Pickle形式）として保存され、適用フェーズで再利用される

#### 2. メトリクス取得モジュール
- `pg_stat_all_tables`、`pg_stat_user_indexes`、`pg_class` などのシステムカタログから統計・構造情報を収集
- `reltuples`、`n_dead_tup`、カーディナリティ、相関係数などを収集対象とする
- クエリに含まれるすべてのテーブルが対象となる

#### 3. 統計更新・実行計画収集モジュール
- 推定されたサンプリング行数をもとに `ANALYZE` を実行し、統計情報を更新
- 続いて対象クエリを実行し、得られた実行計画と実行時間を記録
- 結果は履歴管理テーブルまたはログファイルに保存され、学習に再利用される


### 適用フェーズにおける構成

#### 4. 学習済みモデル適用モジュール
- 学習済みモデルファイルを読み込み、対象テーブルに対してサンプリング行数を推定
- モデルと特徴量に不整合がある場合は既定値にフォールバックする設計とする（フェールセーフ）

#### 5. サンプリング実行・統計更新モジュール（本番適用）
- 推定されたサンプリング行数に従って、`ANALYZE` を実行
- 対象はクエリに含まれるテーブルに限定される
- 統計情報の更新後、実行計画を取得することなく、本番処理を継続可能

### 共通機能

#### 6. ログ・エラーハンドリング機構
- 実行結果や異常検知のログ出力
- エラー発生時のリトライ処理、対象除外、学習フェーズへのフィードバック処理を含む

#### 7. （将来的な構想）Web UI / API連携モジュール
- クエリパターンや予測モデルの可視化
- サンプリング精度や効果の分析画面
- バックエンドAPI連携によるCI/CD統合の検討



---


# 機能要件

本ツールが満たすべき機能要件は以下の通りである。

### 1. サンプリング行数の推定
- 各テーブルに対し、過去の実行ログや統計情報をもとに最適なサンプリング行数を推定する。
- 初期フェーズではルールベースまたは回帰モデルを用い、将来的には強化学習への対応も視野に入れる。

### 2. メトリクスの収集

本ツールでは、初期段階として以下の統計情報およびメタ情報を PostgreSQL から収集対象とする：

- 使用するシステムカタログ：
  - `pg_stat_all_tables`
  - `pg_stat_user_indexes`
  - `pg_class`
  - `pg_attribute`
  - `pg_stats`

- 収集する主な項目：
  - テーブル行数（`reltuples`）
  - 空き領域や断片化の程度（`n_dead_tup`）
  - カーディナリティ、相関係数、`n_distinct` などの列単位の情報

なお、今後の実装過程や性能検証を通じて、必要に応じて他のカタログビューや内部統計情報の収集も視野に入れている。たとえば `pg_stat_progress_analyze` や `pg_stat_io`、拡張統計情報（extended statistics）などが対象候補として挙げられる。


### 3. ANALYZEの実行制御
- 推定された行数に応じて `ANALYZE` を動的に実行する。
- パーティションテーブルや部分インデックスを考慮し、適切なスコープで `ANALYZE` を発行する。
- 対象外テーブル（foreign tablesなど）はスキップする。

### 4. ログ記録および異常検知
- 実行結果（推定値、実行時間、更新結果など）をログとして記録する。
- エラー発生時には適切にログ出力し、再実行やスキップ処理を実装する。

### 5. 設定ファイルおよびパラメータ制御
- ユーザーが対象スキーマや対象外テーブル、閾値などを設定ファイルで指定できるようにする。
- 実行モード（dry-run、実行モードなど）を切り替え可能にする。

### 6. スケジューラやCI/CDとの連携（将来的機能）
- cronやジョブスケジューラとの連携を想定したバッチ実行機能。
- CI/CDツールと連携した自動化の仕組みの構築。



---


# 非機能要件

本ツールの非機能要件は、主に実行効率・保守性・運用性・拡張性を考慮して定義される。  
また、基本設計段階で必要となる各種しきい値や制約条件についても初期値を設け、将来的な自動調整や運用最適化の基盤とする。

## 1. パフォーマンス

- 大規模なテーブルに対しても、数分以内で統計情報の収集と更新が完了することを目指す。
- サンプリングによる高速な統計収集を前提とし、必要に応じてフルスキャンとの比較も行う。
- 実行時のメモリ消費量やCPU使用率を監視し、過剰なリソース消費を回避する。

### 初期しきい値（パフォーマンス関連）

| 項目 | 説明 | 初期値（例） |
|------|------|-------------|
| 最大許容実行時間(ANALYZE) | 1テーブルの処理時間上限。これを超えた場合は警告ログを出力し、処理自体は継続する。※1 | 60秒 |
| メモリ使用上限 | プロセス単位のメモリ制限 | 1GB |
| 並列実行数 | 同時ANALYZE数 | 1～4スレッド |

※1：今後の検討により、処理中断（キャンセル）や通知レベルの変更（エラー扱い）を選択可能にするオプションの追加も視野に入れる。



## 2. 可用性・安定性

- エラー発生時にはログを記録しつつ、安全に処理を終了・スキップする機構を持つ。
- 予期しない異常終了を防ぐフェールセーフ機構を実装する。

### 初期しきい値（異常検知・リトライ）

| 項目 | 説明 | 初期値（例） |
|------|------|-------------|
| リトライ回数 | 一時的な失敗の再試行 | 最大3回 |
| 統計収集失敗時の対応 | 無効な統計が返った場合 | スキップ＋ログ出力 |



## 3. 保守性

- モジュール単位での変更が容易な構成とする。
- ログ出力やエラー内容を確認しやすく、トラブルシューティングが容易であること。
- ※ モジュール構成やinterface分離方針、DIの適用範囲などは、詳細設計にて補足予定。


## 4. 拡張性

- PostgreSQL以外のRDBMSへの対応や、新たな統計項目の追加が可能な構成とする。
- 将来的なWeb UI連携や外部API連携に備えたモジュール分離を意識した設計とする。
- ※ 拡張を見越した抽象化構成やインタフェース定義方針等は、詳細設計にて補足予定。


## 5. 実行環境と互換性

- 初期リリースは PostgreSQL 15系 + Linux環境 でのローカル実行を前提とする。
- 将来的にクラウド環境（例：AWS RDS、GCP Cloud SQL）への対応も視野に入れる。



## 6. セキュリティ

| 項目 | 内容 | 備考 |
|------|------|------|
| SQLインジェクション対策 | 全てのクエリでバインド変数を使用。文字列連結によるSQL構築を禁止。 | GUI展開時は特に注意が必要なため、共通API層で制御を徹底。 |
| サーバパラメータ制限 | `SET`文などでセッション単位のパラメータ変更が行われないよう制限。 | 接続ユーザーの権限で制御。GUI経由でも無効化する設計とする。 |
| 危険コマンド制御 | `COPY`, `DO`, `EXECUTE`などの実行を制限する。 | フロントエンドから渡されるコマンドもバックエンドで検証。 |
| 長時間クエリ対策 | `statement_timeout`の設定により、暴走クエリを防止。 | GUI経由での実行時もタイムアウトを明示的に設ける予定。 |

※ 将来的にGUIによる操作機能を実装予定であり、その際は不正な入力や想定外の操作によるリスクが増加する可能性がある。よって、初期段階からセキュリティ方針を明文化し、バックエンドでの制限を前提とした設計を行う。GUI実装時にはこれらのポリシーに基づき、入力チェック、API認可、ロール制御などを強化予定。



## 7. ロギング・監査

- 実行結果、処理対象、処理時間、推定サンプリング値などをログとして記録。
- 後続分析やパフォーマンス評価のためのCSV出力なども検討する。

### 初期しきい値（ログ関連）

| 項目 | 説明 | 初期値（例） |
|------|------|--------------|
| 実行ログ出力条件 | クエリ実行時間に応じてログ出力 | 5秒超過時のみ出力（※1） |

※1：初期状態では5秒をスロークエリのしきい値とし、それを超えるクエリのみログ出力。将来的には動的調整や平均応答時間に応じたしきい値制御の拡張も想定。また、以下の条件では実行時間に関係なくログ出力を行う：  
・クエリ実行時にエラーが発生した場合  
・統計精度劣化が検出された場合（例：想定されていた `Index Scan` が `Seq Scan` や `Bitmap Heap Scan` に変化した場合、推定コストが過去実績に比べて大幅に上昇した場合など）

#### ▼ 統計精度劣化の検出例（参考）：
- **スキャン方式の変化**  
  例：`Index Scan` → `Seq Scan` / `Bitmap Heap Scan` への変更
- **推定コストの急増**  
  過去の `total cost` や実行時間と比較して倍以上などの増加
- **実行計画上の行数推定誤差**  
  `rows=10` の想定に対して実際は `10000 rows` 返るケースなど（選択性の誤推定）
- **急激な実行時間の増加**  
  クエリ自体が変わっていないにも関わらず、実行時間が数倍に増加


## 8. テーブル選定条件・サンプリング設定

- テーブルの行数やスキーマ、種別によって処理対象を選定する。
- サンプリング率や対象除外条件は、初期値を設けるが将来的にAIで動的に調整予定。

### 初期しきい値（選定・サンプリング関連）

| 項目 | 説明 | 初期値（例） |
|------|------|-------------|
| 最小行数閾値 | 対象とする最低行数 | 設定せず（※1） |
| 除外スキーマ名 | 処理対象から除外 | pg_catalog, information_schema |
| 外部テーブル除外 | 外部テーブルと判定される属性（※2） | 除外対象 |
| サンプリング行数 | ANALYZEで使用されるサンプル数 | PostgreSQLの統計設定（※3）に基づき自動調整 |
| default_statistics_target | 列単位統計の粒度 | 100（PostgreSQL初期値） |
| 統計更新頻度制限 | 再実行を避ける調整 | 1時間 or 1日単位で間引き可（手動設定可）※4 |

※1：データ量が大幅に減少（例：1万件→5千件など）した場合にも統計情報の更新対象としたいため、初期段階ではあえて固定の最小行数閾値は設けない。将来的にはデータ変動量やAIによる動的判定ロジックの導入を想定しており、そのため柔軟性を重視した設計としている。

※2：PostgreSQLでは、`pg_class.relkind` が `'f'` の場合、そのテーブルは「外部テーブル（foreign table）」として定義されている。fdw（foreign data wrapper）経由で外部ソースに接続しているテーブルのため、統計情報の更新対象から除外する。

※3：PostgreSQLでは、`default_statistics_target` の設定値や列数、統計の種類などをもとに、ANALYZE時に必要なサンプリング行数が内部的に決定されます。本ツールではこの仕組みを前提としつつ、必要に応じて調整を検討します。また、将来的にAIによる調整も可能とする設計とします。

※4：将来的には、pg_stat_xxx やクエリ実行時間などを常時監視し、パフォーマンス劣化の兆候をもとに、統計情報の更新タイミングをAI的に自動制御する仕組みへの拡張を想定。


## 備考と補足

- 各しきい値は「初期値」であり、**運用やAIによる動的最適化**を前提とする。
- バージョン・実行環境（オンプレ／クラウド）により変動しやすいため、**環境変数や設定ファイルで上書き可能**にしておくことが望ましい。


---


# 技術選定（使用技術と実行環境）

本ツールの実装にあたっては、以下の観点を踏まえて技術選定を行っている：

- 実行効率と学習コストのバランス
- データ分析・モデル構築に適したライブラリの充実性
- PostgreSQLとの親和性、およびSQLとの連携のしやすさ
- 将来的なWeb UIやクラウド展開の可能性


## 使用技術

| 分類 | 技術 | 用途・理由 |
|------|------|-------------|
| 言語 | Python 3.9+ | データ処理・AIモデル構築・DBアクセスの全てを一貫して実装可能。学習コストとライブラリの豊富さを両立。※1 |
| データ分析 | pandas / numpy | 統計情報や実行ログの加工・分析処理に使用。処理対象が軽量なため、pandas / numpy を選定。※2 |
| 機械学習   | scikit-learn   | 統計情報からサンプリング行数を予測する軽量な回帰モデルの構築に使用。構造がシンプルで小〜中規模データに強く、PoCとしての実装・保守性のバランスが良好なため選定。※3 |
| データ可視化（任意） | matplotlib / seaborn | モデル精度や特徴量分布の確認用途（PoC段階）※4 |
| データベース接続 | psycopg2 | PostgreSQLと安全に連携可能なDBドライバとして採用。 |
| ログ管理 | logging | 実行結果やエラー情報をログ出力。学習用データの蓄積にも利用。 |

※1：RやJuliaはデータ処理や統計解析に優れるが、DBアクセスやバッチ処理との統合性に課題あり。JavaやC#は業務システム向きだが、AI/MLライブラリの使いやすさや学習コストの面で不利。Pythonは機械学習・データ分析・データベース処理の全てにバランス良く対応可能。

※2：クエリ1件あたりの処理対象が小規模（最大100件程度）のため、起動の軽さと成熟したエコシステムを重視して pandas / numpy を選定した。  
処理回数は多くても1回あたりの処理が軽量なため、性能と保守性のバランスが良い。  
Dask や Polars などの高速処理系ライブラリは、大規模データや並列処理向けに設計されており、  
本用途では初期化やスケジューラ起動によるオーバーヘッドが逆に負担となる。文法や保守の観点からも、今回のPoCには適さないと判断した。

※3：本ツールでは、**1回の実行計画取得で扱う特徴量が数十列以下、データ量が数千件程度と小〜中規模**であるため、**軽量で高速な scikit-learn を選定**した。pandas / numpy との親和性や、`.fit()` / `.predict()` の手軽さ、モデルの保存・再利用が容易な点も利点。
XGBoost や LightGBM は高精度だが、本用途では**初期化やチューニングの負荷が高くオーバースペック**。TensorFlow・PyTorch などの深層学習系も、**構築や保守の複雑さ、解釈性の低さから不向き**と判断した。

※4：本ツールでは、モデル精度や特徴量分布の可視化目的で、軽量かつ成熟した matplotlib / seaborn を選定。  
pandas や scikit-learn との親和性が高く、PoC段階で必要な可視化要件を十分に満たせる。  
Plotly や Altair、Dash などの高度な可視化ツールはオーバースペックとなるため、採用しなかった。


## 実行環境

| 区分 | 内容 |
|------|------|
| OS | Linux（Ubuntu 22.04）想定。バッチ処理・データ分析に適した軽量かつ安定した実行環境を提供し、Python / PostgreSQL との親和性・クラウド移行性にも優れる。 macOSも動作対象。 |
| Pythonバージョン | Python 3.9 以上 （主要ライブラリの対応・保守性を考慮）※1|
| PostgreSQLバージョン | PostgreSQL 13 以上（ANALYZE 処理や統計情報管理の機能改善を含む） |
| 実行形式 | ローカルスクリプト（CLI）ベース。cron等による定期実行も可能。 |
| 実行条件 | データベースに接続可能な権限（ANALYZE権限を含む）を持つこと。 |
| 拡張前提 | 将来的に Docker 化、クラウド実行（例：AWS Batch / Lambda）も視野に入れる。 |

※1：Python 3.8以前はすでにセキュリティサポートが終了しており、長期運用の観点からは適さない。また、3.9以降では辞書のマージ演算子 `|` など可読性・保守性を高める新構文が導入されており、学習効率や開発体験の面でも優位性があるため選定。

## 今後の拡張候補

- モデル管理基盤として **MLflow** や **DVC** の導入検討
- 将来的なWeb UI との連携については、REST API を通じて任意のフロントエンド（例：React, Next.js など）と統合できる構成を前提としている。
- モデル自動更新や通知を含む **CI/CD パイプライン連携**


## 処理概要

本ツールは、PostgreSQLにおける統計情報更新処理のパフォーマンス最適化を目的とし、  
AIを用いてテーブルごとに適切なサンプリング行数を推定し、`ANALYZE` 処理に適用することで、更新時間の短縮と最適化を図る。

本ツールは、以下2つのフェーズに分かれて動作する：

- **学習フェーズ（PoC／開発環境想定）**  
　統計情報と実行計画を収集し、最適なサンプリング行数をAIで学習する

- **適用フェーズ（本番想定）**  
　学習済みモデルを用いてサンプリング行数を推定し、`ANALYZE` に適用する


```

[学習フェーズ]
↓
1. クエリを指定
↓
2. クエリに含まれる各テーブルに対して以下を実行：
   - 統計情報の取得
   - AIによるサンプリング行数の推定
   - ANALYZE の実行と統計更新
   - 実行計画と実行時間の取得・記録
↓
3. 実行結果をもとにモデルを再学習し、ファイル保存

↓

[適用フェーズ]
↓
1. クエリを指定
↓
2. テーブルごとにメトリクスを取得
↓
3. 学習済みモデルによりサンプリング行数を推定
↓
4. 推定結果に基づき ANALYZE を実行

```

※詳細なモジュール構成については、[全体構成](#全体構成)を参照。


---

## 9. 入出力仕様

本章では、ツールの主な入出力対象を示す。対象はCLIツールとしての実行に基づき、設定ファイル、DB接続情報、AI推定処理の入出力、ログ等を含む。


### 入力仕様

| 区分 | 内容 | 形式・例 | 備考 |
|------|------|-----------|------|
| クエリ指定 | 対象となるSQLクエリ（学習／適用フェーズ共通） | `SELECT * FROM sales WHERE amount > 1000;` | 設定ファイル or スクリプト内で指定（柔軟に変更可能）※1 |
| 対象DB情報 | DB接続情報（host, port, user, dbname等） | JSON または .env | セキュリティ上、平文保存禁止を推奨 |
| サンプリング条件 | サンプリング対象となるテーブルの設定 | YAML/JSON形式 | 除外スキーマや閾値含む |
| 学習済みモデル | 適用フェーズにおけるモデルファイル | `best_model.pkl` | 学習フェーズ後に生成されたファイル |
| 統計情報メトリクス | PostgreSQLシステムカタログから取得 | `pg_stat_all_tables`, `pg_stats`等 | 自動取得されるため、事前指定不要 |
| 実行モード | 処理の適用方法を制御するモード（通常実行など） | `--mode run` | dry-run的な挙動も将来的に視野に入れる。※2 |


※1：実行時の再現性・精度を担保するため、クエリの指定方法は状況に応じて柔軟に選択可能とする。  
　環境要因による影響を考慮しており、詳細な実行条件や構成は本設計書では割愛する。

※2：現時点では処理適用の有無をPythonロジック側で制御する方式に限定されており、  
　統計情報の更新を完全に抑止する dry-run 機能は一部制約がある。  
　今後、実行計画取得のみに限定したモードや、更新対象のログ出力のみを行う機能も検討中。



### 出力仕様

| 区分 | 内容 | 形式・例 | 備考 |
|------|------|-----------|------|
| 実行ログ | 処理結果や推定行数などを含むログ | `execution_20240624.log` | ログローテーション・圧縮対応を検討 |
| 推定結果 | テーブルごとの推定サンプリング行数 | CSV or JSON | `table_name,estimated_rows` 形式 |
| 統計更新結果 | ANALYZE 実行の成功／失敗、実行時間など | JSON or ログ内埋込 | `{"status":"success","time":2.13}` |
| 実行計画・統計履歴 | 実行クエリに対する EXPLAIN 結果など | 保存先：CSV or DBテーブル | モデル学習に再利用される |
| エラー出力 | 処理中のエラー情報 | stderr またはログファイル | リトライ対象か否かを判定するフラグ含む |
| モデルファイル | 学習済み回帰モデルなど | `best_model.pkl` | scikit-learn joblib / pickle形式想定 |


### 備考

- 入出力ファイルの形式はすべてUTF-8で統一
- モデルファイルや統計結果の出力先は `config.yaml` または CLI で変更可能
- クラウド連携（S3等）を行う場合、出力先URIに `s3://` 指定なども対応可能（拡張予定）

---

## モジュール構成

本章では、本ツールにおける主要な処理フェーズ（学習フェーズ／適用フェーズ）を支えるモジュール構成について説明する。  
各モジュールは役割ごとに明確に分離されており、将来的な機能拡張や差し替えが容易な構造を目指す。


### 学習フェーズ（PoC・開発環境）

| モジュール名 | 役割 |
|--------------|------|
| `metrics_collector.py` | クエリに含まれるテーブルの統計情報（pg_stat系、pg_class等）を収集 |
| `sampling_estimator.py` | 統計情報をもとに、サンプリング行数をAI（回帰モデル）で推定 |
| `analyze_executor.py` | 推定値をもとにANALYZEを実行し、統計情報を更新 |
| `plan_logger.py` | クエリ実行後に実行計画と処理時間を取得・記録 |
| `model_trainer.py` | 実行結果をもとに回帰モデルを学習し、モデルファイル（.pkl）として保存 |



### 適用フェーズ（本番または運用想定）

| モジュール名 | 役割 |
|--------------|------|
| `metrics_collector.py` | 学習フェーズと同様にメトリクスを収集 |
| `model_loader.py` | 学習済みのモデルファイルを読み込み、推定処理に利用 |
| `analyze_executor.py` | 推定されたサンプリング行数でANALYZEを実行（dry-runモードも視野） |



### 共通・補助モジュール

| モジュール名 | 役割 |
|--------------|------|
| `logger.py` | 実行結果、異常系、推定結果のログ出力を担う共通ログ処理 |
| `config_loader.py` | 設定ファイル（YAML/JSON形式）の読み込みと共通パラメータ管理 |
| `query_parser.py` | 渡されたクエリからテーブルを抽出し、対象候補を整理 |
| `error_handler.py` | リトライ・スキップ・通知などのエラーハンドリング処理 |
| `constants.py` | しきい値・固定値などの定義と一元管理 |



### 拡張予定（将来的な構想）

| モジュール名（仮） | 役割 |
|------------------|------|
| `api_server.py` | Web UIや外部ツールからの呼び出しを受け付けるREST APIサーバ |
| `ui_adapter.py` | モデル結果や統計情報を可視化するUI出力変換処理 |
| `job_scheduler.py` | 定期実行や依存ジョブ管理を行うスケジューラ連携モジュール（cron/CI/CD対応） |



※ 各モジュールの責務は詳細設計フェーズにて更に細分化・明確化される予定。  
※ Pythonのパッケージ構成（例：`/modules/`, `/utils/` 等のディレクトリ設計）や依存関係の詳細は、製造・詳細設計フェーズにて整理・調整を行う予定。


---

## 考慮すべきデータ構造と仕様

本章では、統計情報の収集・推定・適用において考慮すべきPostgreSQLの内部構造やカラム特性、およびAIによる行数推定に必要となるデータ仕様について整理する。


### 1. PostgreSQLの統計情報関連構造

| テーブル／ビュー | 説明 | 主に使用するカラム例 |
|------------------|------|----------------------|
| `pg_class` | テーブルの基本情報 | `reltuples`, `relpages`, `relkind` |
| `pg_stat_all_tables` | テーブルごとの統計情報 | `n_tup_ins`, `n_tup_upd`, `n_tup_del`, `n_live_tup`, `n_dead_tup` |
| `pg_stat_user_indexes` | インデックス統計 | `idx_scan`, `idx_tup_read` |
| `pg_attribute` | 各カラムの定義 | `attname`, `attnum`, `attstattarget` |
| `pg_stats` | 列ごとの統計情報（プレーンビュー） | `null_frac`, `n_distinct`, `most_common_vals`, `histogram_bounds`, `correlation` |



### 2. AIモデル学習における特徴量（特徴ベクトル）

AIがサンプリング行数を推定するためには、以下のような特徴量を使用することを想定している：

- テーブルサイズに関する指標
  - `reltuples`（想定行数）
  - `relpages`（データサイズ）

- カラムの統計特性
  - `n_distinct`（ユニーク度）
  - `correlation`（並びの相関）
  - `null_frac`（NULL率）
  - 列数（特徴量の次元数としても活用）

- 更新頻度やデータの変動性
  - `n_tup_upd`, `n_tup_ins`, `n_tup_del`
  - `n_dead_tup`（断片化の度合い）

- その他のメタ情報（設計段階）
  - パーティションテーブルの有無
  - 部分インデックスの有無
  - スキーマ名、テーブル名の命名規則（除外判定用）


### 3. 考慮すべき仕様・制約事項

| 項目 | 説明 | 補足 |
|------|------|------|
| 外部テーブルの扱い | `pg_class.relkind = 'f'` のテーブルは対象外 | FDW経由のテーブルはANALYZE非対応 |
| パーティション構造 | 親子テーブルの関係によってANALYZE対象が異なる | `ANALYZE parent_table` で子が対象になるかどうかに注意 |
| 統計情報の更新粒度 | PostgreSQLは列単位で統計情報を管理 | `default_statistics_target` により調整可能 |
| extended statistics | PostgreSQL 10以降の複数列統計 | 現状では対象外だが、将来的に考慮する可能性あり |
| reltuplesの精度 | 実際の件数とズレが生じる場合あり | ANALYZE後でなければ信頼性が低いケースもある |
| pg_stat_xxx の遅延 | バッファキャッシュの反映遅延に注意 | 特に直後の更新・取得で差が出る可能性あり |
| INHERITS テーブル | PostgreSQL の旧式継承構造（INHERITS）は対象外 | 実運用では推奨されず、パーティションと誤認されるリスクあり |

---

### 4. 今後の拡張に備えたデータ構造想定

- 実行履歴ログ用の構造（CSV または DBテーブル）
  - `query_id`, `table_name`, `estimated_rows`, `actual_duration`, `plan_type`, `timestamp`

- 特徴量キャッシュの仕組み
  - 同一テーブル構造に対して特徴量再計算を省略可能とする設計

- モデル再学習用の教師データ構成
  - 入力（特徴量）＋正解ラベル（最適サンプル数）形式のログ保存



※ 実装段階では、上記仕様をもとにSQLビュー・Python側DataFrame構造として再整理を行う。  
　また、PostgreSQLバージョン差異や拡張統計の扱いについては、検証を通じて導入可否を判断する。



---

## 処理フローとロジック概要

本章では、前章「全体構成」で定義した **学習フェーズ** および **適用フェーズ** に基づき、  
各フェーズにおける処理の流れと、主なロジックの概要を示す。


### 学習フェーズ：処理フローとロジック概要

学習フェーズは、開発環境やPoC環境を前提とし、  
AIが最適なサンプリング行数を学習するための一連の処理を実行する。

#### 処理フロー

1. クエリの指定
2. クエリに含まれるテーブルを抽出
3. 各テーブルについて以下を繰り返し実行：
   - システムカタログから統計情報・構造情報を取得（`pg_stat_all_tables`, `pg_class` 等）
   - AI によるサンプリング行数の推定
   - 推定行数を使用して `ANALYZE` を実行
   - 対象クエリの実行計画と処理時間を記録
4. 実行結果をもとにAIを再学習
5. 学習済みモデル（例：`best_model.pkl`）として保存

#### ロジック概要

- 特徴量には reltuples、n_dead_tup、カーディナリティ、相関係数などを使用
- AIロジックには単純な回帰モデルまたはルールベースを適用（将来的に強化学習等に拡張予定）
- 異常値の除外や最低／最大サンプル数の制約を設けて安全性を担保
- モデル保存時には特徴量スキーマも併せて記録（不整合防止のため）



### 適用フェーズ：処理フローとロジック概要

適用フェーズでは、本番環境において学習済みモデルを活用し、  
指定されたクエリの対象テーブルに対して適切な `ANALYZE` を実行する。

#### 処理フロー

1. クエリの指定
2. クエリに含まれるテーブルを抽出
3. 各テーブルについて以下を実行：
   - メトリクス（統計・構造情報）の取得
   - 学習済みモデルに基づくサンプリング行数の推定
   - 推定行数を使用して `ANALYZE` を実行  
     ※ dry-run オプション時はログ出力のみ

#### ロジック概要

- モデル読み込み時に、特徴量スキーマとの不整合があれば安全な既定値にフォールバック
- 複数テーブルに対応した並列実行をサポート（最大同時実行数は設定可能）
- 推定結果はログに記録し、dry-run 時は JSON または CSV 形式で出力
- 本番環境では実行計画の取得は行わず、ANALYZE のみ実行することで影響を最小化



### 共通機能・補足事項

- **ログ・エラー処理**：各ステップの処理結果、エラー、異常判定ログを記録し、学習フェーズへのフィードバックにも活用
- **閾値管理**：最小／最大サンプリング行数のしきい値を設定し、極端な推定値を自動調整
- **拡張性**：
  - Web UI からの操作や結果可視化を将来的に検討
  - モデルのCI/CD連携や自動評価機能の導入も視野に入れる


---

##  外部インターフェース（DB/API等）

本章では、本ツールが連携・依存する外部インターフェースについて整理する。  
主に PostgreSQL データベースとの接続仕様、および将来的に検討されているAPI連携の構想を示す。



### 1. データベースインターフェース（PostgreSQL）

#### 接続仕様

| 項目 | 内容 |
|------|------|
| 接続対象 | PostgreSQL 13 以上 |
| 接続方式 | `psycopg2` ライブラリによる直接接続（PostgreSQLとの親和性と安定性を重視） |
| 認証方式 | ユーザー名／パスワード（設定ファイルまたは環境変数） |
| 使用ポート | 5432（変更可能） |
| 接続要件 | `ANALYZE` 実行権限、該当スキーマへのSELECT権限、（必要に応じて）INSERT / UPDATE / DELETE 権限 |
| 対象外 | 外部テーブル（FDW）、INHERITSベースの継承テーブル |

#### 主なSQLインターフェース

| 機能 | 使用SQL・ビュー | 備考 |
|------|------------------|------|
| 統計情報取得 | `pg_stat_all_tables`, `pg_class`, `pg_stats`, `pg_attribute` 等 | クエリ対象テーブルに限定して取得 |
| 実行計画取得 | `EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON)` | 学習フェーズのみ実行 |
| 統計情報更新 | `ANALYZE テーブル名` | 推定行数に応じて制御（全体 or 列単位） |
| テーブル情報取得 | `information_schema.tables`, `pg_namespace` 等 | 除外対象の判定に使用（スキーマ名・種別） |



### 2. モデル入出力インターフェース

| 項目 | 内容 |
|------|------|
| 保存形式 | Pickle または Joblib（scikit-learn 互換） |
| 保存タイミング | 学習フェーズ終了時にモデルファイルを保存 |
| 保存場所 | ローカルファイルシステム（初期）、将来的にS3等のクラウドストレージも視野 |
| 読み込み方式 | 適用フェーズ起動時に対象モデルファイルをロード |
| ファイル名構成 | `best_model.pkl`, `model_<timestamp>.pkl` など（切替管理を想定） |



### 3. 将来的なAPI／外部連携構想

#### API連携（想定）

| 機能 | 説明 |
|------|------|
| クエリ実行要求 | API経由でクエリを受け取り、サンプリング推定および `ANALYZE` を実行 |
| モデル可視化 | 学習済みモデルの特徴量重要度、構成などを返すAPI |
| 統計ログ出力 | 統計情報の履歴や推定値のログをJSONで返す |

#### Web UIとの接続想定

- フロントエンドはReactなどの軽量Webフレームワークを想定
- バックエンドはFlaskまたはFastAPI等で構成予定
- サンプル推定結果のグラフ表示、ログモニタリング画面などを構想



### 4. セキュリティ・権限に関する補足

- 接続ユーザーは、ANALYZEの実行権限を含む適切なDB権限を持つこと
- SQLインジェクション対策として、すべてのクエリにはパラメータバインドを使用
- ファイル保存パスは設定ファイルまたは環境変数から取得し、ハードコーディングは避ける
- 将来的にAPI連携を行う場合は、API認証・ロール制御・アクセス制限の導入が必須となる

---


## AIによるサンプリング行数の推定ロジック

本章では、統計情報更新処理において、各テーブルに対して適切なサンプリング行数を推定するためのAIロジックの設計方針と実装概要を示す。

### 1. 目的

- `ANALYZE` 実行時における **過剰な全件スキャンの抑制** と、**統計精度の最適化** の両立を図る。
- テーブルごとに適切なサンプリング行数を推定し、処理時間と精度のバランスを最適化することを目的とする。



### 2. 推定手法

#### 基本手法

| 項目 | 内容 |
|------|------|
| モデル種別 | 回帰モデル（線形回帰、ランダムフォレスト、LightGBM 等を検証予定） |
| 入力（特徴量） | テーブル統計情報、構造情報、更新頻度、カラム統計など |
| 出力 | 推定されたサンプリング行数（整数値） |
| 実装ライブラリ | `scikit-learn`（初期段階）、将来的に `XGBoost`, `LightGBM` 等への切替も検討 |



### 3. 特徴量の例

以下の特徴量をベースとし、AIモデルに入力する：

- **テーブル全体統計**
  - `reltuples`（行数想定）
  - `relpages`（ブロック数）

- **更新・断片化**
  - `n_tup_upd`, `n_tup_ins`, `n_tup_del`, `n_dead_tup`

- **カラム統計**
  - `n_distinct`, `null_frac`, `correlation`（相関係数）

- **構造的特徴**
  - パーティション有無、インデックスの有無、カラム数



### 4. 推定値の制御と補正

- 最小・最大サンプリング行数にしきい値を設定し、過剰または極端な推定値を防止
- 以下のいずれかの条件に該当する場合は、既定値（default_statistics_target = 100 ※）にフォールバックする：
  - 特徴量取得に失敗した場合
  - モデルバージョンと特徴量スキーマが一致しない場合
  - 推定値が設定しきい値を大きく外れる場合

※ default_statistics_target = 100 の場合、内部的には最大で1カラムあたり約1万〜3万行程度がサンプリングされます（PostgreSQLの仕様に基づく）。


### 5. モデルの運用と更新

| 項目 | 内容 |
|------|------|
| 保存形式 | `joblib` または `pickle` |
| 保存単位 | モデルファイル（例：`best_model.pkl`）＋スキーマ情報（JSON形式） |
| 再学習 | 学習フェーズで定期的にログから再学習可能 |
| モデル切替 | タイムスタンプ or バージョン番号付きファイルで管理し、明示的な切替に対応 |



### 6. 備考

- 特徴量の設計とモデリングは拡張性を持たせ、将来的な精度改善やモデル種別の切替に柔軟に対応できる構造とする。
- 異常値やデータスパイクに対するロバスト性を確保するため、特徴量の正規化や外れ値補正のロジックも今後検討。


必要に応じて、CIパイプラインによるモデル精度検証や、モデル評価指標（RMSE、R² など）の記録も導入可能とする。
